# ğŸ“ MCQ Generator using Groq LLM

An AI-powered web application that generates **high-quality multiple-choice questions (MCQs)**, **document summaries**, and **learning guides** from **PDFs or raw text** using **Groq Cloud LLMs**.  
Designed to simulate **human-level exam question creation** with adjustable difficulty and a clean, interactive quiz interface.

---

## ğŸš€ Project Overview

This project transforms unstructured educational content (PDFs or text) into a **complete learning and assessment experience**:

- Upload a **PDF** or paste **text**
- Choose **difficulty level** (Easy / Medium / Hard)
- Generate **context-aware MCQs**
- Attempt the quiz interactively
- Export the quiz as a **PDF**
- Get a **summary + curated learning roadmap**

The system leverages **Groqâ€™s ultra-fast inference** with **LLaMA 3.3 (70B)** to ensure high contextual accuracy and low latency.

---

## ğŸ§  Key Features

- ğŸ“„ **PDF Upload & Text Parsing**
- âœï¸ **Human-like MCQ Generation**
- ğŸ¯ **Difficulty Control** (Easy / Medium / Hard)
- ğŸ“ **Interactive Clickable Quiz UI**
- ğŸ“Œ **Concise Document Summary**
- ğŸ“š **Learning Guide & Resource Suggestions**
- ğŸ“¤ **Export Quiz as PDF**
- âš¡ **Fast Response Time (< 5 seconds)**

---

## ğŸ—ï¸ Tech Stack & Tools

### Backend
- **Python**
- **Flask** â€“ Web framework
- **Groq Cloud API**
- **LLaMA 3.3 70B Versatile** (LLM)

### Frontend
- **HTML5**
- **CSS3** (custom responsive styling)

### NLP & Utilities
- **Prompt Engineering**
- **PyPDF2** â€“ PDF text extraction
- **Regex-based MCQ parsing**
- **ReportLab** â€“ PDF export

### Dev & Deployment
- **Git & GitHub**
- **Virtual Environment (venv)**
- **REST-style architecture**

---
User
â†“
HTML/CSS UI
â†“
Flask Backend
â”œâ”€â”€ PDF/Text Ingestion
â”œâ”€â”€ Prompt Engineering Layer
â”œâ”€â”€ Groq LLM (LLaMA 3.3 70B)
â”‚ â”œâ”€â”€ Summary Generation
â”‚ â”œâ”€â”€ MCQ Generation
â”‚ â””â”€â”€ Learning Guide Creation
â”œâ”€â”€ MCQ Parsing Engine
â””â”€â”€ PDF Export Module
â†“
Formatted Quiz & Learning Output


---

## ğŸ“Š Performance Metrics (Observed)

- **Contextual Accuracy:** ~87% (manual evaluation on academic PDFs)
- **Average Response Time:** 3â€“5 seconds
- **MCQ Quality:** Conceptual & reasoning-based (not shallow recall)
- **Scalability:** Supports user-defined number of questions

> Note: Accuracy is based on qualitative evaluation of relevance, correctness, and conceptual depth.

---

## ğŸ§ª Example Use Cases

- ğŸ“˜ Generating quizzes from **research papers**
- ğŸ“ Exam preparation from **lecture notes**
- ğŸ§  Self-assessment while learning **ML / NLP / CS topics**
- ğŸ« Faculty or coaching material generation
- ğŸ“„ Quick understanding of long PDFs

---

## ğŸ§© Project Structure

mcq_generator/
â”‚
â”œâ”€â”€ app.py # Flask app
â”œâ”€â”€ groq_client.py # Groq API wrapper
â”œâ”€â”€ prompts.py # Prompt engineering logic
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ pdf_reader.py # PDF text extraction
â”‚ â”œâ”€â”€ mcq_parser.py # Structured MCQ parsing
â”‚ â””â”€â”€ pdf_exporter.py # Export quiz to PDF
â”‚
â”œâ”€â”€ templates/
â”‚ â””â”€â”€ index.html # Frontend UI
â”‚
â”œâ”€â”€ static/
â”‚ â””â”€â”€ style.css # Styling
â”‚
â”œâ”€â”€ uploads/ # Temporary PDF storage
â””â”€â”€ .env # API keys (ignore)
ğŸš€ Future Enhancements

Auto-evaluation & scoring

Timer-based quizzes

User authentication & quiz history

Deployment on Render / AWS

Chunking for very large PDFs

Analytics dashboard

